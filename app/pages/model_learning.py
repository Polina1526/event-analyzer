import json
import time

import constants
import data_processing
import pandas as pd
import pydantic
import streamlit as st
import utils
from catboost import CatBoostClassifier
from sklearn.metrics import average_precision_score, roc_auc_score
from sklearn.model_selection import train_test_split
from tsfresh import select_features


st.set_page_config(page_title="–û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏", page_icon="üìñ")


def get_data_preprocessing_settings(event_options: list[str], max_days_in_data: int) -> tuple[str, int]:
    with st.container(border=True):
        st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –¥–∞–Ω–Ω—ã—Ö")
        col1, col2 = st.columns(2)
        with col1:
            _target_event_name: list = st.multiselect(
                label="–í—ã–±–µ—Ä–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ —Å–æ–±—ã—Ç–∏—è", options=event_options, max_selections=1
            )
        with col2:
            window_days: int = st.number_input(
                label="–í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ —Ü–µ–ø–æ—á–∫–∏ —Å–æ–±—ã—Ç–∏–π (–≤ –¥–Ω—è—Ö)",
                min_value=1,
                max_value=max_days_in_data,
                value=min(constants.DEFAULT_WINDOW_DAYS, max_days_in_data),
                step=1,
            )
    if not _target_event_name or not window_days:
        st.stop()
    return _target_event_name[0], window_days


class SidebarSettings(pydantic.BaseModel):
    data_count: int
    feature_extruction_chunksize: int
    training_validation_size: float
    training_catboost_iterations: int
    training_catboost_depth: int
    training_catboost_learning_rate: float


def create_sidebar(threadid_count: int) -> SidebarSettings:
    data_count: int = st.sidebar.number_input(
        label="–ö–æ–ª-–≤–æ —Ü–µ–ø–æ—á–µ–∫, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏",
        min_value=2,
        max_value=threadid_count,
        value=min(constants.DEFAULT_DATA_COUNT, threadid_count),
        step=1,
    )
    feature_extruction_chunksize: int = st.sidebar.number_input(
        label="–ö–æ–ª-–≤–æ —Ü–µ–ø–æ—á–µ–∫ –≤ –æ–¥–Ω–æ–º —á–∞–Ω–∫–µ –ø—Ä–∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–µ",
        min_value=1,
        max_value=threadid_count,
        value=min(constants.DEFAULT_CHUNKSIZE, threadid_count),
        step=1,
    )
    training_validation_size: float = st.sidebar.number_input(
        label="–î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏",
        min_value=0.0,
        max_value=1.0,
        value=constants.DEFAULT_VALIDATION_SIZE,
        step=0.05,
    )
    training_catboost_iterations: int = st.sidebar.number_input(
        label="–ö–æ–ª-–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏",
        min_value=0,
        max_value=100000,
        value=constants.DEFAULT_CATBOOST_ITERATIONS,
        step=1,
    )
    training_catboost_depth: int = st.sidebar.number_input(
        label="–ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤ –≤ –±—É—Å—Ç–∏–Ω–≥–µ", min_value=0, max_value=100, value=constants.DEFAULT_CATBOOST_DEPTH, step=1
    )
    training_catboost_learning_rate: float = st.sidebar.number_input(
        label="Laerning rate", min_value=0.0, max_value=1.0, value=constants.DEFAULT_CATBOOST_LEARNING_RATE, step=0.001
    )
    return SidebarSettings(
        data_count=data_count,
        feature_extruction_chunksize=feature_extruction_chunksize,
        training_validation_size=training_validation_size,
        training_catboost_iterations=training_catboost_iterations,
        training_catboost_depth=training_catboost_depth,
        training_catboost_learning_rate=training_catboost_learning_rate,
    )


def limit_data_size(df: pd.DataFrame, data_count: int, random_state: int = 42) -> pd.DataFrame:
    return df[
        df[constants.THREADID_COL_NAME].isin(
            pd.Series(df[constants.THREADID_COL_NAME].unique()).sample(data_count, random_state=random_state)
        )
    ]


@st.cache_data(show_spinner="–û—Ç–±–æ—Ä –ø–æ–ª–µ–∑–Ω—ã—Ö —Ñ–∏—á–µ–π")
def _select_features(df: pd.DataFrame, y: pd.Series) -> pd.DataFrame:
    return select_features(df, y)


@st.cache_data(show_spinner=False)
def learn_model(
    features: pd.DataFrame,
    y: pd.DataFrame,
    iterations: int,
    depth: int,
    learning_rate: float,
    test_size: float = 0.2,
    random_state: int = 42,
) -> tuple[CatBoostClassifier, pd.DataFrame, list[str]]:
    X_train, X_test, y_train, y_test = train_test_split(features, y.y, test_size=test_size, random_state=random_state)

    main_features: list[str] = list(_select_features(df=X_train, y=y_train).columns)
    X_train = X_train[main_features]
    X_test = X_test[main_features]

    model = CatBoostClassifier(
        iterations=iterations,
        depth=depth,
        learning_rate=learning_rate,
        loss_function="Logloss",
        eval_metric="AUC",
        verbose=False,
        allow_writing_files=False,
    )
    with st.spinner("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"):
        model.fit(X_train, y_train, eval_set=(X_test, y_test), use_best_model=True)

    with st.spinner("–ü–æ–¥—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫ –Ω–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"):
        probas_X_train = model.predict_proba(X_train)[:, 1]
        probas_X_test = model.predict_proba(X_test)[:, 1]

    metrics: dict[str, list[str]] = {
        "–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞": [
            f"{roc_auc_score(y_train, probas_X_train):.4f}",
            f"{average_precision_score(y_train, probas_X_train):.4f}",
        ],
        "–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞": [
            f"{roc_auc_score(y_test, probas_X_test):.4f}",
            f"{average_precision_score(y_test, probas_X_test):.4f}",
        ],
    }
    return model, pd.DataFrame.from_dict(metrics, orient="index", columns=["ROC-AUC", "PR-AUC"]), main_features


class ModelLearningResults(pydantic.BaseModel):
    learning_time: float
    feature_extraction_time: float
    target_event_name: str
    window_days: int
    learning_metrics: dict[str, str]
    data_file_name: str
    settings: dict
    main_features: list[str]


def _prepare_learning_metrics_for_saving(metrics_df: pd.DataFrame) -> dict[str, str]:
    metrics_series = metrics_df.stack()
    metrics_series.index = metrics_series.index.map(" ".join)
    return metrics_series.to_dict()


def model_learning_page() -> None:
    st.title("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ—Ç–æ–∫–∞–º–∏ —Å–æ–±—ã—Ç–∏–π")
    st.sidebar.header("–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")

    raw_data: pd.DataFrame
    file_name: str
    raw_data, file_name = utils.load_train_data(label="–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –º–æ–¥–µ–ª–∏")
    raw_data = data_processing.column_standardization(raw_data)

    target_event_name: str
    window_days: int
    target_event_name, window_days = get_data_preprocessing_settings(
        event_options=raw_data[constants.EVENT_TYPE_COL_NAME].unique(),
        max_days_in_data=(
            raw_data[constants.TIMESTAMP_COL_NAME].max() - raw_data[constants.TIMESTAMP_COL_NAME].min()
        ).days,
    )

    processed_data: pd.DataFrame
    y_to_id_mapping: pd.DataFrame
    processed_data, y_to_id_mapping, _ = data_processing.preprocess_data(
        df=raw_data, target_event=target_event_name, time_window=f"{window_days}D"
    )

    sidebar_settings: SidebarSettings = create_sidebar(
        threadid_count=processed_data[constants.THREADID_COL_NAME].nunique()
    )

    processed_data = limit_data_size(df=processed_data, data_count=sidebar_settings.data_count)
    # —Ç—É—Ç –º–æ–∂–Ω–æ –≤—Å—Ç–∞–≤–∏—Ç—å –∏–Ω—Ñ—É –æ —Ç–æ–º, —Å–∫–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ  # TODO

    with st.form("learning_start_form"):
        model_name: str = st.text_input(label="–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        form_send: bool = st.form_submit_button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏")
    if not form_send:
        st.stop()

    features: pd.DataFrame
    y: pd.DataFrame
    start_feature_extraction_time = time.time()
    features, y = data_processing.feature_extraction(
        df=processed_data, chunksize=sidebar_settings.feature_extruction_chunksize, y_to_id_mapping=y_to_id_mapping
    )
    execution_feature_extraction_time: float = time.time() - start_feature_extraction_time
    st.write(f"–í—Ä–µ–º—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö: {execution_feature_extraction_time: 0.4f} —Å–µ–∫—É–Ω–¥")

    model: CatBoostClassifier
    metrics_df: pd.DataFrame
    main_features: list[str]
    start_learning_time = time.time()
    model, metrics_df, main_features = learn_model(
        features=features,
        y=y,
        test_size=sidebar_settings.training_validation_size,
        iterations=sidebar_settings.training_catboost_iterations,
        depth=sidebar_settings.training_catboost_depth,
        learning_rate=sidebar_settings.training_catboost_learning_rate,
    )
    execution_learning_time = time.time() - start_learning_time

    st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
    st.write(f"–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {execution_learning_time: 0.4f} —Å–µ–∫—É–Ω–¥")
    st.write(metrics_df)

    with st.spinner(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_name}.cbm"):
        model.save_model(constants.MODEL_SAVING_PATH.format(model_name))

        with open(constants.MODEL_PIVOT_TABLE_PATH, mode="r") as file:
            model_pivot_table_dict: dict[str, dict] = json.load(file)

        model_pivot_table_dict[model_name] = ModelLearningResults(
            learning_time=execution_learning_time,
            feature_extraction_time=execution_feature_extraction_time,
            target_event_name=target_event_name,
            window_days=window_days,
            learning_metrics=_prepare_learning_metrics_for_saving(metrics_df),
            data_file_name=file_name,
            settings=sidebar_settings.model_dump(),
            main_features=main_features,
        ).model_dump()

        with open(constants.MODEL_PIVOT_TABLE_PATH, mode="w") as file:
            json.dump(model_pivot_table_dict, file, indent=4)

    st.success(f"–ú–æ–¥–µ–ª—å {model_name}.cbm —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")


model_learning_page()
